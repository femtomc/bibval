@article{attention2017,
    author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    title = {Attention Is All You Need},
    journal = {Advances in Neural Information Processing Systems},
    year = {2017},
    url = {https://arxiv.org/abs/1706.03762},
}

@article{bert2019,
    author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    journal = {NAACL},
    year = {2019},
    doi = {10.18653/v1/N19-1423},
}

@article{gpt3,
    author = {Tom Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and others},
    title = {Language Models are Few-Shot Learners},
    journal = {NeurIPS},
    year = {2020},
    eprint = {2005.14165},
    archiveprefix = {arXiv},
}

@inproceedings{resnet,
    author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    title = {Deep Residual Learning for Image Recognition},
    booktitle = {CVPR},
    year = {2016},
    doi = {10.1109/CVPR.2016.90},
}

@article{wrong_year,
    author = {John Smith},
    title = {Attention Is All You Need},
    journal = {NeurIPS},
    year = {2020},
}

@article{typo_title,
    author = {Jacob Devlin},
    title = {BERT: Pre-trainning of Deep Bidirectional Transformers},
    journal = {NAACL},
    year = {2019},
}
